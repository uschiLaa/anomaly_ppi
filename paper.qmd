---
title: "Are you normal? A new projection pursuit index to assess a sample against a multivariate null distribution"
author: "Annalisa Calvi, Ursula Laa, Dianne Cook"
format:
  jasa-pdf:
    keep-tex: true    
bibliography: bibliography.bib  
header-includes: | 
  \usepackage{amsmath}
  \usepackage{float}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \usepackage{bm}
  \def\tightlist{}
  \usepackage{setspace}
  \newcommand\pD{$p\text{-}D$}
  \newcommand\kD{$k\text{-}D$}
  \newcommand\dD{$d\text{-}D$}
  \newcommand\gD{$2\text{-}D$}
---

```{r include=FALSE}
# Set up chunk for for knitr
knitr::opts_chunk$set(
  fig.width = 5,
  fig.height = 5,
  fig.align = "center",
  out.width = "100%",
  code.line.numbers = FALSE,
  fig.retina = 4,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE
)
```

```{r}
#| label: load-libraries
#| warning: false
#| echo: false
library(tourr)
library(mulgar)
library(dplyr)
library(ggplot2)
library(patchwork)
```

```{r}
#| label: plot-theme
theme_set(theme_linedraw() +
   theme(
     aspect.ratio = 1,
     plot.background = element_rect(fill = 'transparent', colour = NA),
     plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5),
     panel.background = element_rect(fill = 'transparent', 
                                     colour = NA),
     panel.grid.major = element_blank(), 
     panel.grid.minor = element_blank(), 
     axis.title.x = element_blank(), axis.title.y = element_blank(),
     axis.text.x = element_blank(), axis.ticks.x = element_blank(),
     axis.text.y = element_blank(), axis.ticks.y = element_blank(),
     legend.background = element_rect(fill = 'transparent', 
                                      colour = NA),
     legend.key = element_rect(fill = 'transparent', 
                               colour = NA),
     legend.position = "bottom", 
     legend.title = element_blank(), 
     legend.text = element_text(size=4),
     legend.key.height = unit(0.25, 'cm'),
     legend.key.width = unit(0.25, 'cm')
   )
)
interior_annotation <- function(label, position = c(0.92, 0.92)) {
  annotation_custom(grid::textGrob(label = label,
      x = unit(position[1], "npc"), y = unit(position[2], "npc"),
      gp = grid::gpar(cex = 1, col="grey70")))
}
```

## Introduction

Linear projections are useful in many aspects of statistical analysis of multivariate data, and especially useful for visualising the data. A linear projection provides a dimension reduction while maintaining interpretability. For example, a biplot (REF) shows the structure creating the maximum variance in the data, and also visualizing the projection matrix to learn which variables contribute to it. We might find clusters of outliers that were hiding in high dimensions.

More generally, projection pursuit (REF) defines a quantitative criterion for the interestingness of a projection (a projection pursuit index), and searches the space of possible projections for the most interesting one to display. We can also define sequences of interpolated linear projections to better understand a multivariate distribution. Animating a randomly selected interpolated sequence of linear projections is called a grand tour (REF). The combination of these two approaches would then use a projection pursuit index to select interesting projections, but display them via an interpolated path to provide context. This is called a guided tour (@cook1995).

The question is whether we can use these techniques to assess new data samples in the context of an established normal, such as a specific multivariate normal distribution. In physics, the normal distribution may describe experimental results, or a global fit for a selected model, and we might want to compare to a set of other models. In medical applications, the normal distribution might summarize historic data of a healthy population and we compare it to samples from new patients. In outlier detection we might use robust measures to define the normal distribution and look for anomalies. 

This paper describes a new projection pursuit index which is optimized by projections where a new sample is most distant from the existing normal distribution. It is organised as follows. @sec-background provides more context for the methods and visualisation. @sec-anomaly-index provides the details of the new index, and example use is illustrated in @sec-examples. 

## Background {#sec-background}

To compare a new sample with an existing norm, like a multivariate normal distribution, in higher than two dimensions, we have typically used two samples of points. The norm is represented by points on the surface of a $p$-dimensional ellipsoid, corresponding to a confidence level. A sample of points uniformly distributed on a $p$-dimensional sphere is generated by 

1. Simulating a sample of observations ($\mathbfit{x}$, which are \pD{} vectors)
from $N_p(\mathbfit{\mu}, \Sigma)$. 
2. Transforming each observation to have unit distance from the mean, $\frac{\mathbfit{x}^\top}{||\mathbfit{x}^\top||}$. 

To convert this to points on the surface of a confidence ellipsoid, 

3. transform the shape using a specific variance covariance, and shift to center on the mean vector. 

Finally, new observations can be visually compared with this ellipsoid by 

4. plotting them together. 

@fig-ci illustrates this process for \gD. This is easiest way to view this normal region relative to a new sample for any \pD{} problem. 

```{r}
#| echo: false
#| label: fig-ci
#| fig-width: 8
#| fig-height: 2
#| fig-cap: "Simulating a uniform sample on a sphere involves sampling from a multivariate normal (a) and transforming each observation to have length equal to 1. A confidence ellipsoid is generated by transforming the sphere relative to a specified variance-covariance matrix (c), and new observations can be visually assessed to be inside or outside by plotting with the ellipsoid (d)."
set.seed(717)
d1 <- matrix(rnorm(716 * 2), ncol = 2) 
d2 <- t(apply(d1, 1, geozoo:::l2norm_vec))
vc <- matrix(c(1, -0.5, -0.5, 1), ncol=2, byrow=TRUE)
evc <- eigen(vc)
vc2 <- (evc$vectors) %*% diag(sqrt(evc$values)) %*% t(evc$vectors)
d3 <- d2 %*% vc2 |>
  as.data.frame()

d4 <- d3 |>
  bind_rows(data.frame(V1=0.6, V2=0.6)) |>
  bind_rows(data.frame(V1=0.3, V2=0.3)) |>
  mutate(type = c(rep("ci", nrow(d3)), "in", "out"))

d1 <- as.data.frame(d1)
d2 <- as.data.frame(d2)
d3 <- as.data.frame(d3)

p1 <- ggplot(d1, aes(x=V1, y=V2)) +
  geom_point(alpha=0.5) +
  xlab(expression(x[1])) +
  ylab(expression(x[2])) +
  xlim(c(-3.2, 3.2)) +
  ylim(c(-3.2, 3.2)) +
  interior_annotation("a") +
  #ggtitle("a. Bivariate standard normal sample") +
  theme(axis.text = element_blank())

p2 <- ggplot(d2, aes(x=V1, y=V2)) +
  geom_point(alpha=0.5) +
  xlab(expression(x[1])) +
  ylab(expression(x[2])) +
  xlim(c(-1.2, 1.2)) +
  ylim(c(-1.2, 1.2)) +
  interior_annotation("b") +
  #ggtitle("b. Uniform on a sphere") +
  theme(axis.text = element_blank())

p3 <- ggplot(d3, aes(x=V1, y=V2)) +
  geom_point(alpha=0.5) +
  xlab(expression(x[1])) +
  ylab(expression(x[2])) +
  xlim(c(-1.2, 1.2)) +
  ylim(c(-1.2, 1.2)) +
  interior_annotation("c") +
  #ggtitle("c. Points on surface of confidence ellipsoid.") +
  theme(axis.text = element_blank())

p4 <- ggplot(d4, aes(x=V1, y=V2, shape=type)) +
  geom_point(alpha=0.5) +
  xlab(expression(x[1])) +
  ylab(expression(x[2])) +
  xlim(c(-1.2, 1.2)) +
  ylim(c(-1.2, 1.2)) +
  interior_annotation("d") +
  #ggtitle("d. Observations inside and outside confidence ellipsoid.") +
  theme(legend.position = "none", 
        axis.text = element_blank())
  
p1 + p2 + p3 + p4 + plot_layout(ncol=4)
```

<PLOTS FROM USCHI's PHYSICS EXAMPLE HERE>

Although this is flexible, this does not make it easy to guide the tour towards the directions (projections) where the samples are most different from the normal. What would be desirable is to analytically define the confidence ellipsoid, compute flag observations that are outside, steer the tour to projections that reveal the extent of the difference. And also display the projected ellipsoid as a geometric shape rather than a sample of points. These are the procedures that are described in the next section.  

## Anomaly index {#sec-anomaly-index}

## Projecting an ellipsoid

A \pD{} ellipsoid corresponding to a given variance-covariance ($\Sigma$) is defined by 

$$
(\mathbfit{x}-\mathbfit{\mu}) \Sigma^{-1}(\mathbfit{x}-\mathbfit{\mu})^T = c^2
$$
where $c$ a constant that depends on a specific confidence level.

The projection of an ellipsoid onto \gD{} is an ellipse, where the curve of the ellipse is defined through the set of points $\mathbfit{x}$ for which the gradient is parallel to the projection plane. We call points in the projection that are on the curve $\mathbfit{y}$. From this we can compute the analog equation for the projection as

$$(\mathbfit{y} - \mathbfit{\mu}_p)(P^T \Sigma P)^{-1}(\mathbfit{y} - \mathbfit{\mu}_p)^T = c^2$$
where $P$ is a $(p\times 2)$ orthonormal basis defining the projection and $\mathbfit{\mu}_p = \mathbfit{\mu} P$ the projected mean. This means the matrix $(P^T \Sigma P)^{-1}$ is defining the ellipse in the \gD{} projection. In general $c$ could be any constant, but typically we would select it as a quantile of the $\chi^2$ distribution, so that the size of the ellipse corresponds to a selected probability.

<IMAGES OF PROJECTED ELLIPSE>

### Index specification

To define a measure of an interesting projection is to maximize the **average Malahanobis distance in the projection** for a subset of points, $W$. The set of points could be chosen in different ways, but the default is those that are outside the specified ellipsoid in \pD{}. Alternatives could be to select a set of observations with the largest Mahalanobis distance, manually select observations or possibly a group of points identified via clustering of the extremes.

The index is written as

$$
\sum_{\mathbfit{w} \in W} (\mathbfit{w} - \mathbfit{\mu}) P (P^T\Sigma P)^{-1}P^T(\mathbfit{w} - \mathbfit{\mu})^T
$$

where by default $W = \{\mathbfit{x}: (\mathbfit{x}-\mathbfit{\mu}) \Sigma^{-1}(\mathbfit{x}-\mathbfit{\mu})^T > c^2\}$, is the set of observations outside the \pD{} ellipsoid.

### Additional considerations

If the observations in $W$ are primarily departing from the normal range in the same direction, the index will be expected to perform well in finding this average direction. However, if the observations have very different departures from the norm, it may be useful to break them into groups, and separately optimize on these subsets. One could consider clustering these observations using angular distance to find groups of observations that have similar directions of departure.

## Implementation

This is implemented in the `tourr` (REF) package, where the projected ellipsoid can be drawn for each projection. The guided tour will take arguments specifying the data, and the null variance-covariance matrix. 

```{r}
#| eval: false
#| echo: true
library(tourr)
library(mulgar)
set.seed(929)
vc_null <- matrix(rep(0.5, 5*5), ncol=5)
diag(vc_null) <- 1
m_null <- rep(0, 5)
vc_samp <- matrix(rep(0, 5*5), ncol=5)
diag(vc_samp) <- 1
vc_samp[4,5] <- -0.47
vc_samp[5,4] <- -0.56
vc_samp <- vc_samp*0.1
m_samp <- c(0, 0, 0, 1.9, 2.3)
samp <- as.data.frame(rmvn(6, 
                           mn = m_samp, 
                           vc = vc_samp))
animate_xy(samp, guided_anomaly_tour(anomaly_index(),
  ellipse=vc_null), ellipse=vc_null, 
  axes = "bottomleft", half_range=5, center=FALSE)
```

::: {#fig-anomaly layout-ncol=2}

![Random projection.](images/anomaly1.png){#fig-anomaly1 width=300 align="center"}
 
![Optimal projection.](images/anomaly2.png){#fig-anomaly2 width=300 align="center"}

Two projections of simulated example data corresponding to the sample code: (a) random projection where sample is inside the 2-D ellipse, (b) optimal projection from index, showing most of the sample outside. A red cross indicates that the point is outside the p-D ellipse. The optimal projection uses mostly variables $x_4, x_5$, which is expected because these are the two directions where the sample most differs from the norm.
:::

## Examples {#sec-examples}

### Medical patients

### Robust statistics

### Physics?


## Conclusion

